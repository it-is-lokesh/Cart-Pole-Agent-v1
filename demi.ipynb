{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm as tq\n",
    "from PIL import Image\n",
    "import PIL.ImageDraw as ImageDraw\n",
    "import imageio\n",
    "import os\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCLearning:\n",
    "    def __init__(self, eps=0.1, use_pretrained=True):\n",
    "        self.x = np.round(np.linspace(-0.3, 0.3, num=30), decimals=3)\n",
    "        self.x_dot = np.round(np.linspace(-1.5, 1.5, num=15), decimals=3)\n",
    "        self.th = np.round(np.linspace(-0.21, 0.21, num=21), decimals=3)\n",
    "        self.th_dot = np.round(np.linspace(-2.5, 2.5, num=25), decimals=3)\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        self.actions = np.array([0, 1])\n",
    "        self.run_hist = []\n",
    "        self.eps = eps\n",
    "        self.name = \"Monte_carlo\"\n",
    "        if use_pretrained:\n",
    "            self.policy = np.load('./MCLearning/custom_reward_1/policy.npy')\n",
    "            self.st_act_val = np.load(\n",
    "                './MCLearning/custom_reward_1/st_act_val.npy')\n",
    "            with open('./MCLearning/custom_reward_1/st_dict_idx.pkl', 'rb') as f:\n",
    "                self.st_dict_idx = pickle.load(f)\n",
    "            with open('./MCLearning/custom_reward_1/st_idx.pkl', 'rb') as f:\n",
    "                self.st_idx_dict = pickle.load(f)\n",
    "            with open('./MCLearning/custom_reward_1/run_hist.pkl', 'rb') as f:\n",
    "                self.run_hist = pickle.load(f)\n",
    "            self.state_count = self.policy.shape[0]\n",
    "            print(\"Resuming training from\", len(self.run_hist), \"episode!\")\n",
    "\n",
    "    def generate_states(self):\n",
    "        self.states = []\n",
    "        for x in self.x:\n",
    "            for x_dot in self.x_dot:\n",
    "                for th in self.th:\n",
    "                    for th_dot in self.th_dot:\n",
    "                        self.states.append((x, x_dot, th, th_dot))\n",
    "        '''\n",
    "        The structure of self.state_dict_idx: key is the state tuple, value is its index\n",
    "        The structure of self.state_idx_dict\n",
    "        '''\n",
    "        self.st_dict_idx = {state: i for i, state in enumerate(self.states)}\n",
    "        self.st_idx_dict = {i: state for i, state in enumerate(self.states)}\n",
    "        '''\n",
    "        The variable self.policy states the best action for each state, taking the state's index\n",
    "        '''\n",
    "        self.state_count = len(self.states)\n",
    "        self.policy = np.random.randint(0, 2, (self.state_count))\n",
    "\n",
    "        '''\n",
    "        The structure of self.st_act_val is as follows:\n",
    "        1. The length of this array depicts the config of each state.\n",
    "        2. The two vectors in each state correspond to the two actions - 0, 1\n",
    "        3. The first value for each action vector is the action value for that state\n",
    "        4. The second value for each action vector is the freq with which that action is taken\n",
    "        '''\n",
    "        self.st_act_val = np.zeros((self.state_count, 2, 2))\n",
    "        del self.states\n",
    "\n",
    "    def get_state(self, desc):\n",
    "        def nearest(v, x):\n",
    "            if x <= v[0]:\n",
    "                return v[0]\n",
    "            elif x >= v[-1]:\n",
    "                return v[-1]\n",
    "            n = len(v)\n",
    "            lo, hi, mid = 0, n-1, 0\n",
    "            while (lo < hi):\n",
    "                mid = lo+int((hi-lo)/2)\n",
    "                if v[mid] == x:\n",
    "                    return v[mid]\n",
    "                if x < v[mid]:\n",
    "                    if (mid > 0 and x > v[mid - 1]):\n",
    "                        return v[mid] if v[mid]-x >= v[mid-1]-x else v[mid-1]\n",
    "                    hi = mid\n",
    "                else:\n",
    "                    if (mid < n - 1 and x < v[mid + 1]):\n",
    "                        return v[mid] if v[mid]-x >= v[mid+1]-x else v[mid+1]\n",
    "                    lo = mid + 1\n",
    "            return v[mid]\n",
    "        x = nearest(self.x, desc[0])\n",
    "        x_dot = nearest(self.x_dot, desc[1])\n",
    "        th = nearest(self.th, desc[2])\n",
    "        th_dot = nearest(self.th_dot, desc[3])\n",
    "        return self.st_dict_idx[(x, x_dot, th, th_dot)]\n",
    "\n",
    "    # Using e-greedy to evaluate policy\n",
    "    def get_action(self, state):\n",
    "        rn = np.random.uniform(0, 1)\n",
    "        act_best = self.policy[self.get_state(state)]\n",
    "        if rn > self.eps:\n",
    "            return act_best\n",
    "        else:\n",
    "            return np.random.choice([0, 1])\n",
    "\n",
    "    def save_agent(self):\n",
    "        np.save('./MCLearning/custom_reward_1/policy.npy', self.policy)\n",
    "        np.save('./MCLearning/custom_reward_1/st_act_val.npy', self.st_act_val)\n",
    "        with open('./MCLearning/custom_reward_1/st_dict_idx.pkl', 'wb') as f:\n",
    "            pickle.dump(self.st_dict_idx, f)\n",
    "        with open('./MCLearning/custom_reward_1/st_idx.pkl', 'wb') as f:\n",
    "            pickle.dump(self.st_idx_dict, f)\n",
    "        with open('./MCLearning/custom_reward_1/run_hist.pkl', 'wb') as f:\n",
    "            pickle.dump(self.run_hist, f)\n",
    "        plt.plot(self.run_hist)\n",
    "        plt.savefig('./MCLearning/custom_reward_1/run_hist.png')\n",
    "\n",
    "    # Using every time visit\n",
    "    def train(self, episodes=20):\n",
    "        self.state_freq = np.zeros(self.state_count)\n",
    "        for episode in tq(range(episodes)):\n",
    "            history = []\n",
    "            obs, _ = self.env.reset()\n",
    "            while (1):\n",
    "                action = self.get_action(obs)\n",
    "                state = self.get_state(obs)\n",
    "                obs, reward, term, trunc, _ = self.env.step(action)\n",
    "                reward += (1-obs[0]**2-obs[2]**2)\n",
    "                history.append((state, action, reward))\n",
    "                if term or trunc:\n",
    "                    break\n",
    "\n",
    "            self.run_hist.append(len(history))\n",
    "\n",
    "            cum_reward = 0\n",
    "            for step in reversed(history):\n",
    "                st, act, rew = step\n",
    "                cum_reward += rew\n",
    "                self.st_act_val[st][act][1] += 1\n",
    "                self.st_act_val[st][act][0] += (\n",
    "                    cum_reward - self.st_act_val[st][act][0])/self.st_act_val[st][act][1]\n",
    "\n",
    "            ### Policy Improvement ###\n",
    "            for st in range(self.state_count):\n",
    "                tmp = np.array([self.st_act_val[st][0][0],\n",
    "                               self.st_act_val[st][1][0]])\n",
    "                self.policy[st] = np.argmax(tmp)\n",
    "            if episode % 100 == 99:\n",
    "                self.save_agent()\n",
    "        self.save_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from 2 episode!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 47/20000 [00:41<4:52:32,  1.14it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\RL\\20IE10028_Sai_Lokesh_Gorantla\\demi.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/RL/20IE10028_Sai_Lokesh_Gorantla/demi.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m agent \u001b[39m=\u001b[39m MCLearning(eps\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, use_pretrained\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/RL/20IE10028_Sai_Lokesh_Gorantla/demi.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m agent\u001b[39m.\u001b[39mgenerate_states()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/RL/20IE10028_Sai_Lokesh_Gorantla/demi.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m agent\u001b[39m.\u001b[39;49mtrain(episodes\u001b[39m=\u001b[39;49m\u001b[39m20000\u001b[39;49m)\n",
      "\u001b[1;32md:\\RL\\20IE10028_Sai_Lokesh_Gorantla\\demi.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/RL/20IE10028_Sai_Lokesh_Gorantla/demi.ipynb#W2sZmlsZQ%3D%3D?line=127'>128</a>\u001b[0m \u001b[39mfor\u001b[39;00m st \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate_count):\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/RL/20IE10028_Sai_Lokesh_Gorantla/demi.ipynb#W2sZmlsZQ%3D%3D?line=128'>129</a>\u001b[0m     tmp \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mst_act_val[st][\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m],\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/RL/20IE10028_Sai_Lokesh_Gorantla/demi.ipynb#W2sZmlsZQ%3D%3D?line=129'>130</a>\u001b[0m                    \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mst_act_val[st][\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]])\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/RL/20IE10028_Sai_Lokesh_Gorantla/demi.ipynb#W2sZmlsZQ%3D%3D?line=130'>131</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy[st] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49margmax(tmp)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/RL/20IE10028_Sai_Lokesh_Gorantla/demi.ipynb#W2sZmlsZQ%3D%3D?line=131'>132</a>\u001b[0m \u001b[39mif\u001b[39;00m episode \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m99\u001b[39m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/RL/20IE10028_Sai_Lokesh_Gorantla/demi.ipynb#W2sZmlsZQ%3D%3D?line=132'>133</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_agent()\n",
      "File \u001b[1;32mc:\\Users\\gsail\\miniconda3\\envs\\rl\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1229\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1142\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1143\u001b[0m \u001b[39mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[0;32m   1144\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[39m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1227\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m kwds \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mkeepdims\u001b[39m\u001b[39m'\u001b[39m: keepdims} \u001b[39mif\u001b[39;00m keepdims \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39m_NoValue \u001b[39melse\u001b[39;00m {}\n\u001b[1;32m-> 1229\u001b[0m \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39m\u001b[39margmax\u001b[39m\u001b[39m'\u001b[39m, axis\u001b[39m=\u001b[39maxis, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\gsail\\miniconda3\\envs\\rl\\lib\\site-packages\\numpy\\core\\fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     60\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[39m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[39m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[39m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[39m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = MCLearning(eps=-1, use_pretrained=True)\n",
    "agent.generate_states()\n",
    "agent.train(episodes=20000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
